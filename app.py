# app.py (vers√£o 3 - final e correta)

import os
import streamlit as st
from dotenv import load_dotenv

# Importa√ß√µes necess√°rias de LangChain
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_huggingface import HuggingFaceEndpoint # Importa√ß√£o do Endpoint
from langchain_huggingface import ChatHuggingFace     # Importa√ß√£o do Adaptador de Chat

# Carrega as vari√°veis de ambiente (sua chave da Hugging Face)
load_dotenv()

# --- Constantes ---
VECTOR_STORE_PATH = "vector_store/"
EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'
LLM_REPO_ID = "meta-llama/Meta-Llama-3-8B-Instruct" 

# --- Fun√ß√µes ---

def create_rag_chain():
    """
    Cria a cadeia de RAG (Retrieval-Augmented Generation) completa.
    """
    # Carrega o banco de dados vetorial salvo localmente
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    try:
        vector_store = FAISS.load_local(VECTOR_STORE_PATH, embeddings, allow_dangerous_deserialization=True)
    except Exception as e:
        st.error(f"Erro ao carregar o Vector Store: {e}")
        st.error("Verifique se voc√™ executou 'ingest.py' e se a pasta 'vector_store' n√£o est√° vazia.")
        return None

    retriever = vector_store.as_retriever()

    # PASSO 1: Criar o Endpoint base que se conecta ao Hugging Face.
    base_llm = HuggingFaceEndpoint(
        repo_id=LLM_REPO_ID,
        max_new_tokens=512,
        temperature=0.7,
        huggingfacehub_api_token=os.getenv("HUGGINGFACEHUB_API_TOKEN"),
    )

    # PASSO 2: Envolver o endpoint no adaptador ChatHuggingFace.
    # O argumento obrigat√≥rio 'llm' recebe o nosso endpoint base.
    llm = ChatHuggingFace(llm=base_llm)

    # Template de prompt otimizado para modelos de chat
    system_prompt = """Voc√™ √© um assistente de IA focado em responder perguntas com base em um contexto fornecido.
    Use os trechos de texto do contexto para responder √† pergunta do usu√°rio.
    Se a resposta n√£o estiver no contexto, diga claramente que voc√™ n√£o encontrou a informa√ß√£o nos documentos.
    N√£o tente inventar uma resposta. Seja direto e √∫til.

    Contexto:
    {context}
    """
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{question}"),
    ])

    # Fun√ß√£o auxiliar para formatar os documentos recuperados em uma √∫nica string
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    # Construindo a cadeia com a LangChain Expression Language (LCEL)
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain

# --- Interface Gr√°fica com Streamlit ---

st.set_page_config(page_title="Assistente de Documentos PDF", page_icon="ü§ñ")

st.title("ü§ñ Assistente Conversacional para Documentos PDF")
st.markdown("""
Esta aplica√ß√£o permite que voc√™ converse com seus documentos PDF.
1.  Coloque seus arquivos PDF na pasta `data`.
2.  Execute o script `ingest.py` para process√°-los.
3.  Fa√ßa suas perguntas abaixo e obtenha respostas baseadas nos documentos.
""")

# Verifica se o Vector Store existe antes de iniciar o chat
if not os.path.exists(VECTOR_STORE_PATH) or not os.listdir(VECTOR_STORE_PATH):
    st.error("O diret√≥rio do Vector Store n√£o foi encontrado ou est√° vazio. Por favor, execute o script 'ingest.py' primeiro.")
else:
    # Cria a cadeia de RAG
    rag_chain = create_rag_chain()

    if rag_chain:
        # Campo de entrada para a pergunta do usu√°rio
        user_question = st.text_input("Fa√ßa sua pergunta:")

        if user_question:
            with st.spinner("Buscando a resposta..."):
                response = rag_chain.invoke(user_question)
                
                st.subheader("Resposta:")
                st.write(response)